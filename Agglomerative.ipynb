{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Agglomerative Clustering"
      ],
      "metadata": {
        "id": "oJ32HDeBc766"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import functools\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from itertools import combinations\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jzEsVtDHdEfn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hh_mm_ss2seconds(hh_mm_ss):\n",
        "    return functools.reduce(lambda acc, x: acc*60 + x, map(int, hh_mm_ss.split(':')))"
      ],
      "metadata": {
        "id": "g2uq1_f6dRmK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing Data"
      ],
      "metadata": {
        "id": "a0C8KUCmdK-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to datasets\n",
        "datasets = ['./Data/set1.csv', './Data/set2.csv', './Data/set3noVID.csv']\n",
        "\n",
        "# Standardize features\n",
        "features = ['SEQUENCE_DTTM', 'LAT', 'LON', 'SPEED_OVER_GROUND', 'COURSE_OVER_GROUND']\n",
        "\n",
        "def process_dataset(file_path):\n",
        "    df = pd.read_csv(file_path, converters={'SEQUENCE_DTTM' : hh_mm_ss2seconds})\n",
        "    X = df[features]\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(X)\n",
        "\n",
        "# Load and pre-process each dataset\n",
        "processed_data = [process_dataset(file) for file in datasets]"
      ],
      "metadata": {
        "id": "RzxY6W3adWUW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection\n",
        "\n",
        "Use set1 & set2 to determing which features to choose"
      ],
      "metadata": {
        "id": "bUIuU4Z5da5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "\n",
        "# Features for set1 and set2\n",
        "X1 = processed_data[0]\n",
        "X2 = processed_data[1]\n",
        "\n",
        "# Labels for set1 and set2\n",
        "y1 = pd.read_csv('./Data/set1.csv')['VID']\n",
        "y2 = pd.read_csv('./Data/set2.csv')['VID']\n",
        "\n",
        "num_clusters = 20  # unique VID in set1 and set2\n",
        "\n",
        "features = range(X1.shape[1])\n",
        "best_score = 0\n",
        "best_combo = None\n",
        "\n",
        "for i in range(1, len(features) + 1):\n",
        "    for combo in combinations(features, i):\n",
        "        # Select the features for this combination for both sets\n",
        "        X1_subset = X1[:, combo]\n",
        "        X2_subset = X2[:, combo]\n",
        "\n",
        "        # Agglomerative clustering on Set 1\n",
        "        agglo1 = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "        predictions1 = agglo1.fit_predict(X1_subset)\n",
        "        score1 = adjusted_rand_score(y1, predictions1)\n",
        "\n",
        "        # Agglomerative clustering on Set 2\n",
        "        agglo2 = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "        predictions2 = agglo2.fit_predict(X2_subset)\n",
        "        score2 = adjusted_rand_score(y2, predictions2)\n",
        "\n",
        "        # Calculate the average ARI\n",
        "        average_score = (score1 + score2) / 2\n",
        "        print(f\"Testing features {combo}: Average ARI = {average_score} | set1: {score1:.4f} | set2: {score2:.4f}\")\n",
        "\n",
        "        # Check if this combination gave a better average score\n",
        "        if average_score > best_score:\n",
        "            best_score = average_score\n",
        "            best_combo = combo\n",
        "\n",
        "print(f\"Best Average ARI: {best_score} with features {best_combo}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnpfC2DXddle",
        "outputId": "0d79c09a-8062-4126-df68-f2bf4faee3fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing features (0,): Average ARI = 0.052937951694586136 | set1: 0.0177 | set2: 0.0881\n",
            "Testing features (1,): Average ARI = 0.09406632127986468 | set1: 0.1276 | set2: 0.0605\n",
            "Testing features (2,): Average ARI = 0.10181886290730681 | set1: 0.1434 | set2: 0.0602\n",
            "Testing features (3,): Average ARI = 0.23316536106074354 | set1: 0.1062 | set2: 0.3601\n",
            "Testing features (4,): Average ARI = 0.225805940708544 | set1: 0.1252 | set2: 0.3265\n",
            "Testing features (0, 1): Average ARI = 0.1474341179111534 | set1: 0.1165 | set2: 0.1784\n",
            "Testing features (0, 2): Average ARI = 0.17661706609228706 | set1: 0.1210 | set2: 0.2322\n",
            "Testing features (0, 3): Average ARI = 0.2006922846079403 | set1: 0.0949 | set2: 0.3065\n",
            "Testing features (0, 4): Average ARI = 0.2137897841906293 | set1: 0.1315 | set2: 0.2961\n",
            "Testing features (1, 2): Average ARI = 0.12910161654152197 | set1: 0.1467 | set2: 0.1115\n",
            "Testing features (1, 3): Average ARI = 0.24857829045230118 | set1: 0.1373 | set2: 0.3598\n",
            "Testing features (1, 4): Average ARI = 0.2189037403755573 | set1: 0.2219 | set2: 0.2160\n",
            "Testing features (2, 3): Average ARI = 0.2028364571723367 | set1: 0.1581 | set2: 0.2475\n",
            "Testing features (2, 4): Average ARI = 0.21203496655420723 | set1: 0.2122 | set2: 0.2119\n",
            "Testing features (3, 4): Average ARI = 0.3672469623591579 | set1: 0.1995 | set2: 0.5350\n",
            "Testing features (0, 1, 2): Average ARI = 0.19854119592467984 | set1: 0.1446 | set2: 0.2525\n",
            "Testing features (0, 1, 3): Average ARI = 0.21052875416009048 | set1: 0.1412 | set2: 0.2798\n",
            "Testing features (0, 1, 4): Average ARI = 0.23036470915748447 | set1: 0.2014 | set2: 0.2593\n",
            "Testing features (0, 2, 3): Average ARI = 0.2398083566093676 | set1: 0.1592 | set2: 0.3204\n",
            "Testing features (0, 2, 4): Average ARI = 0.2686847988984312 | set1: 0.2117 | set2: 0.3257\n",
            "Testing features (0, 3, 4): Average ARI = 0.29054224911420923 | set1: 0.2028 | set2: 0.3783\n",
            "Testing features (1, 2, 3): Average ARI = 0.23916327807698778 | set1: 0.1588 | set2: 0.3195\n",
            "Testing features (1, 2, 4): Average ARI = 0.26082834469502725 | set1: 0.2620 | set2: 0.2597\n",
            "Testing features (1, 3, 4): Average ARI = 0.2828626840157402 | set1: 0.2578 | set2: 0.3079\n",
            "Testing features (2, 3, 4): Average ARI = 0.2770154669542844 | set1: 0.2766 | set2: 0.2774\n",
            "Testing features (0, 1, 2, 3): Average ARI = 0.24521126216489042 | set1: 0.1779 | set2: 0.3125\n",
            "Testing features (0, 1, 2, 4): Average ARI = 0.27638439796932635 | set1: 0.2461 | set2: 0.3067\n",
            "Testing features (0, 1, 3, 4): Average ARI = 0.2858183680019292 | set1: 0.2428 | set2: 0.3289\n",
            "Testing features (0, 2, 3, 4): Average ARI = 0.3220494749953487 | set1: 0.2369 | set2: 0.4072\n",
            "Testing features (1, 2, 3, 4): Average ARI = 0.29718825497268964 | set1: 0.2872 | set2: 0.3072\n",
            "Testing features (0, 1, 2, 3, 4): Average ARI = 0.28503377794810475 | set1: 0.2737 | set2: 0.2964\n",
            "Best Average ARI: 0.3672469623591579 with features (3, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems that it performs the best with featrues 1, 2 ,3 and 4.\n",
        "\n",
        "Note that max score was acheived using only 3 and 4 but this is because it overperformed on set2"
      ],
      "metadata": {
        "id": "yonct5uZhYCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search"
      ],
      "metadata": {
        "id": "7s5BHcvNiupb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features for set1 and set2\n",
        "X1 = processed_data[0]\n",
        "X2 = processed_data[1]\n",
        "\n",
        "X1 = X1[:, 1:5] # Get featues 3 and 4 only\n",
        "X2 = X2[:, 1:5]\n",
        "\n",
        "\n",
        "# Grid search parameters\n",
        "n_clusters = [20, 25, 30]\n",
        "linkages = ['ward', 'average', 'complete', 'single']\n",
        "affinities = ['euclidean', 'manhattan', 'cosine']\n",
        "\n",
        "# Initialize the best score and combination tracking variables\n",
        "best_score = 0\n",
        "best_params = {}\n",
        "\n",
        "# Function to validate the combination of affinity and linkage\n",
        "def valid_combination(linkage, affinity):\n",
        "    if linkage == 'ward' and affinity != 'euclidean':\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Grid search loop\n",
        "for n_cluster in n_clusters:\n",
        "    for linkage in linkages:\n",
        "        for affinity in affinities:\n",
        "            if not valid_combination(linkage, affinity):\n",
        "                continue  # Skip invalid combinations\n",
        "\n",
        "            # Agglomerative clustering on Set 1\n",
        "            agglo1 = AgglomerativeClustering(n_clusters=n_cluster, linkage=linkage, metric=affinity)\n",
        "            predictions1 = agglo1.fit_predict(X1)\n",
        "            score1 = adjusted_rand_score(y1, predictions1)\n",
        "\n",
        "            # Agglomerative clustering on Set 2\n",
        "            agglo2 = AgglomerativeClustering(n_clusters=n_cluster, linkage=linkage, metric=affinity)\n",
        "            predictions2 = agglo2.fit_predict(X2)\n",
        "            score2 = adjusted_rand_score(y2, predictions2)\n",
        "\n",
        "            # Calculate the average ARI\n",
        "            average_score = (score1 + score2) / 2\n",
        "            print(f\"Testing {n_cluster} clusters, {linkage} linkage, {affinity} affinity: Average ARI = {average_score}\")\n",
        "\n",
        "            # Update the best parameters and score\n",
        "            if average_score > best_score:\n",
        "                best_score = average_score\n",
        "                best_params = {'n_clusters': n_cluster, 'linkage': linkage, 'affinity': affinity}\n",
        "\n",
        "# Output the best parameters and score\n",
        "print(f\"Best Average ARI: {best_score}\")\n",
        "print(f\"Best Parameters: {best_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfYXT_J2iyXK",
        "outputId": "58e6ee38-cbb6-4d26-c439-266b5956b790"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 20 clusters, ward linkage, euclidean affinity: Average ARI = 0.29718825497268964\n",
            "Testing 20 clusters, average linkage, euclidean affinity: Average ARI = 0.3362404018215226\n",
            "Testing 20 clusters, average linkage, manhattan affinity: Average ARI = 0.358682251635327\n",
            "Testing 20 clusters, average linkage, cosine affinity: Average ARI = 0.3691034121942901\n",
            "Testing 20 clusters, complete linkage, euclidean affinity: Average ARI = 0.303116835390497\n",
            "Testing 20 clusters, complete linkage, manhattan affinity: Average ARI = 0.3549237718565946\n",
            "Testing 20 clusters, complete linkage, cosine affinity: Average ARI = 0.4109809179965021\n",
            "Testing 20 clusters, single linkage, euclidean affinity: Average ARI = 0.2090711591489875\n",
            "Testing 20 clusters, single linkage, manhattan affinity: Average ARI = 0.2081610727208823\n",
            "Testing 20 clusters, single linkage, cosine affinity: Average ARI = 0.04743385100631472\n",
            "Testing 25 clusters, ward linkage, euclidean affinity: Average ARI = 0.28058672056595046\n",
            "Testing 25 clusters, average linkage, euclidean affinity: Average ARI = 0.35586670055644487\n",
            "Testing 25 clusters, average linkage, manhattan affinity: Average ARI = 0.35373484322760224\n",
            "Testing 25 clusters, average linkage, cosine affinity: Average ARI = 0.35043981650060574\n",
            "Testing 25 clusters, complete linkage, euclidean affinity: Average ARI = 0.3079950869668326\n",
            "Testing 25 clusters, complete linkage, manhattan affinity: Average ARI = 0.34766850725977266\n",
            "Testing 25 clusters, complete linkage, cosine affinity: Average ARI = 0.33446636714252675\n",
            "Testing 25 clusters, single linkage, euclidean affinity: Average ARI = 0.2091710427282184\n",
            "Testing 25 clusters, single linkage, manhattan affinity: Average ARI = 0.2082601518286083\n",
            "Testing 25 clusters, single linkage, cosine affinity: Average ARI = 0.0479837494271642\n",
            "Testing 30 clusters, ward linkage, euclidean affinity: Average ARI = 0.2628781142731526\n",
            "Testing 30 clusters, average linkage, euclidean affinity: Average ARI = 0.2931950529644561\n",
            "Testing 30 clusters, average linkage, manhattan affinity: Average ARI = 0.26475788945040246\n",
            "Testing 30 clusters, average linkage, cosine affinity: Average ARI = 0.3515683885144007\n",
            "Testing 30 clusters, complete linkage, euclidean affinity: Average ARI = 0.28709952475671685\n",
            "Testing 30 clusters, complete linkage, manhattan affinity: Average ARI = 0.33495193460976475\n",
            "Testing 30 clusters, complete linkage, cosine affinity: Average ARI = 0.3262698285821418\n",
            "Testing 30 clusters, single linkage, euclidean affinity: Average ARI = 0.2085233377505078\n",
            "Testing 30 clusters, single linkage, manhattan affinity: Average ARI = 0.20760064200583564\n",
            "Testing 30 clusters, single linkage, cosine affinity: Average ARI = 0.19158336276564555\n",
            "Best Average ARI: 0.4109809179965021\n",
            "Best Parameters: {'n_clusters': 20, 'linkage': 'complete', 'affinity': 'cosine'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems to perform better than K-means with this combination of hyper-params.\n",
        "\n",
        "Best Parameters: {'n_clusters': 25, 'linkage': 'complete', 'affinity': 'manhattan'}"
      ],
      "metadata": {
        "id": "7QJVLXg7mSp1"
      }
    }
  ]
}